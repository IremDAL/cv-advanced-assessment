name: Build TensorRT Engines

on:
  workflow_dispatch:

jobs:
  build-trt:
    runs-on: ubuntu-latest

    steps:
    - name: Checkout repository
      uses: actions/checkout@v4

    - name: Install system dependencies
      run: |
        sudo apt update
        sudo apt install -y python3-dev python3-pip wget unzip

    - name: Install CUDA 11.8
      run: |
        wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64/cuda-ubuntu2204.pin
        sudo mv cuda-ubuntu2204.pin /etc/apt/preferences.d/cuda-repository-pin-600
        sudo apt-key adv --fetch-keys https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64/3bf863cc.pub
        sudo add-apt-repository "deb https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64/ /"
        sudo apt update
        sudo apt install -y cuda-toolkit-11-8

    - name: Install TensorRT 8.6
      run: |
        wget https://developer.nvidia.com/downloads/compute/machine-learning/tensorrt/secure/8.6.1/local_repo/nv-tensorrt-local-repo-ubuntu2204-8.6.1-cuda-11.8_1.0-1_amd64.deb
        sudo dpkg -i nv-tensorrt-local-repo-ubuntu2204-8.6.1-cuda-11.8_1.0-1_amd64.deb
        sudo cp /var/nv-tensorrt-local-repo-*/nv-tensorrt-local-E3BFCCB.key /usr/share/keyrings/
        sudo apt update
        sudo apt install -y tensorrt python3-libnvinfer-dev uff-converter-tf

    - name: Install Python dependencies
      run: |
        python3 -m pip install onnx onnxruntime-gpu numpy

    - name: Create models folder
      run: mkdir -p models

    - name: Download ONNX model
      run: |
        wget -O models/model.onnx https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8n.onnx

    - name: Build FP16 TensorRT Engine
      run: |
        python3 - << 'EOF'
        import tensorrt as trt
        import numpy as np

        TRT_LOGGER = trt.Logger(trt.Logger.WARNING)
        builder = trt.Builder(TRT_LOGGER)
        network = builder.create_network(1 << int(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH))
        parser = trt.OnnxParser(network, TRT_LOGGER)

        with open("models/model.onnx", "rb") as f:
            parser.parse(f.read())

        config = builder.create_builder_config()
        config.set_flag(trt.BuilderFlag.FP16)
        config.max_workspace_size = 1 << 30  # 1GB

        profile = builder.create_optimization_profile()
        profile.set_shape("images", (1, 3, 640, 640), (1, 3, 640, 640), (1, 3, 640, 640))
        config.add_optimization_profile(profile)

        engine = builder.build_engine(network, config)
        with open("models/model_fp16.engine", "wb") as f:
            f.write(engine.serialize())
        EOF

    - name: Upload artifacts
      uses: actions/upload-artifact@v4
      with:
        name: trt-engines
        path: models/
